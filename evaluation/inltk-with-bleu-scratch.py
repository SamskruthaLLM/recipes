from collections import Counter
import math
from inltk.inltk import tokenize
import pandas as pd


def n_gram_precision(reference_tokens, candidate_tokens, n=1):
    """
    Calculate the precision for n-grams.
    
    Args:
    reference_tokens (list): The reference sentence tokens.
    candidate_tokens (list): The candidate sentence tokens.
    n (int): The n-gram length.
    
    Returns:
    float: The n-gram precision.
    """
    
    ref_ngrams = Counter([tuple(reference_tokens[i:i+n]) for i in range(len(reference_tokens)-n+1)])
    cand_ngrams = Counter([tuple(candidate_tokens[i:i+n]) for i in range(len(candidate_tokens)-n+1)])
    
    # Calculate the number of matching n-grams
    match_count = sum(min(cand_ngrams[gram], ref_ngrams[gram]) for gram in cand_ngrams)
    total_cand_ngrams = max(len(candidate_tokens) - n + 1, 1)
    
    # Precision is the ratio of matching n-grams to total candidate n-grams
    precision = match_count / total_cand_ngrams
    print("Precision: ", precision)
    return precision

def brevity_penalty(reference_tokens, candidate_tokens):
    """
    Calculate the brevity penalty.
    
    Args:
    reference_tokens (list): The reference sentence tokens.
    candidate_tokens (list): The candidate sentence tokens.
    
    Returns:
    float: The brevity penalty.
    """
    ref_len = len(reference_tokens)
    cand_len = len(candidate_tokens)
    
    if cand_len > ref_len:
        return 1
    else:
        return math.exp(1 - ref_len / cand_len)

def calculate_bleu(reference, candidate, max_n=1):
    """
    Calculate the BLEU score between a reference sentence and a candidate sentence.
    
    Args:
    reference (str): The reference sentence.
    candidate (str): The candidate sentence generated by a model.
    max_n (int): The maximum n-gram length to consider (default is 4).
    
    Returns:
    float: The BLEU score.
    """
    
    # Calculate precision for 1-gram to max_n-gram
    precisions = [n_gram_precision(reference_tokens, candidate_tokens, n) for n in range(1, max_n+1)]
    
    # Calculate the geometric mean of the precisions
    geometric_mean = math.exp(sum(math.log(p) if p > 0 else -999999 for p in precisions) / max_n)
    
    # Calculate the brevity penalty
    bp = brevity_penalty(reference_tokens, candidate_tokens)
    
    # Calculate the final BLEU score
    bleu_score = bp * geometric_mean
    
    return bleu_score

dataset = pd.read_csv("./responses-gemma-translation.csv")

ground_truth = dataset["ground_truth"]
response = dataset["finetuned_response"]

bleu_scores = []

i = 0
for reference_sentence,candidate_sentence in zip(ground_truth,response[:101]):
    reference_tokens = tokenize(reference_sentence, "sa")
    candidate_tokens = tokenize(candidate_sentence, "sa")

    bleu_score = calculate_bleu(reference_tokens, candidate_tokens)
    print(i,bleu_score)
    bleu_scores.append(bleu_score)
    i+=1
    if i == 100:
        print(f"Avg BLEU score:",sum(bleu_scores)/len(bleu_scores))

print(f"Avg BLEU score:",sum(bleu_scores)/len(bleu_scores))
